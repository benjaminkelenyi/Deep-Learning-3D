# This repository  is used to centralize the papers in the 3D area.

## Deep-Learning-3D
[Deep-Learning-On-Point-Clouds](https://github.com/PointCloudYC/Deep-Learning-On-Point-Clouds)

## Visual-Transformer-3D-Self-Attention
[Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

[Visual-Transformer-Paper-Summary](https://github.com/AIprogrammer/Visual-Transformer-Paper-Summary)

[Awesome-Visual-Representation-Learning-With-Transformers](https://github.com/alohays/awesome-visual-representation-learning-with-transformers)

### Survey

[Transformers in computational visual media: A survey](https://link.springer.com/content/pdf/10.1007/s41095-021-0247-3.pdf)

[Transformers in Vision: A Survey](https://dl.acm.org/doi/pdf/10.1145/3505244)

## Spatial-Transformer
### 2D
[Spatial Transformer Networks](https://arxiv.org/abs/1506.02025) -> [Code](https://github.com/kevinzakka/spatial-transformer-network), [Notes](https://kevinzakka.github.io/2017/01/10/stn-part1/), [YT](https://www.youtube.com/watch?v=6NOQC_fl1hQ)

### 3D
[Spatial Transformer for 3D Point Clouds](https://arxiv.org/abs/1906.10887) -> [Code](https://github.com/samaonline/spatial-transformer-for-3d-point-clouds), [YT](https://www.youtube.com/watch?v=UAijTLXkupQ)

## Datasets

- [Stanford Bunny](http://graphics.stanford.edu/data/3Dscanrep/): [A Volumetric Method for Building Complex Models from Range Images](https://graphics.stanford.edu/papers/volrange/volrange.pdf) [SIGGRAPH 1996]
- [KITTI](http://www.cvlibs.net/datasets/kitti/):  [Are we ready for autonomous driving? the KITTI vision benchmark suite](http://www.cvlibs.net/publications/Geiger2012CVPR.pdf) [CVPR 2012]
- [NYUV2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html): [Indoor Segmentation and Support Inference
from RGBD Images](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/shkf_eccv2012.pdf) [ECCV 2012]
- [FAUST](http://faust.is.tue.mpg.de/): [FAUST: Dataset and evaluation for 3D mesh registration](http://files.is.tue.mpg.de/black/papers/FAUST2014.pdf) [CVPR 2014]
- [ICL-NUIM](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html): [A Benchmark for RGB-D Visual Odometry, 3D Reconstruction and SLAM](http://mural.maynoothuniversity.ie/8309/1/JM-Benchmark-2014.pdf) [ICRA 2014]
- [Augmented ICL-NUIM](http://redwood-data.org/indoor/dataset.html): [Robust Reconstruction of Indoor Scenes](https://www.researchgate.net/profile/Vladlen_Koltun/publication/279751165_Robust_Reconstruction_of_Indoor_Scenes/links/5599867708ae5d8f393633dc/Robust-Reconstruction-of-Indoor-Scenes.pdf) [CVPR 2015]
- [ModelNet](https://modelnet.cs.princeton.edu): [3d shapenets: A deep representation for volumetric shapes](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wu_3D_ShapeNets_A_2015_CVPR_paper.pdf) [`cls`; CVPR 2015]
- [SUN RGB-D](http://rgbd.cs.princeton.edu/challenge.html): [Sun rgb-d: A rgb-d scene understanding benchmark suite](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_SUN_RGB-D_A_2015_CVPR_paper.pdf) [`det`; CVPR 2015]
- [SHREC15](http://www.cs.cf.ac.uk/shaperetrieval/shrec15/): [SHRECâ€™15 Track: Non-rigid 3D Shape Retrieval](http://www.cs.cf.ac.uk/shaperetrieval/files/Lian_3DOR_2015.pdf) [Eurographics 2015]
- [ShapeNetCore](https://www.shapenet.org/): [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/pdf/1512.03012.pdf) [`cls`; arXiv 2015]
- [ShapeNet Part](https://cs.stanford.edu/~ericyi/project_page/part_annotation/index.html): [A Scalable Active Framework for Region Annotation in 3D Shape Collections](https://www-cs.stanford.edu/~ericyi/papers/part_annotation_16_small.pdf) [`seg`; SIGGRAPH Asia 2016]
- [SceneNN](http://103.24.77.34/scenenn/home/): [SceneNN: A Scene Meshes Dataset with aNNotations](https://www.researchgate.net/profile/Binh-Son_Hua/publication/311758430_SceneNN_A_Scene_Meshes_Dataset_with_aNNotations/links/5a6078300f7e9bfbc3f753f4/SceneNN-A-Scene-Meshes-Dataset-with-aNNotations.pdf) [3DV 2016]
- [Oxford RobotCar](https://robotcar-dataset.robots.ox.ac.uk/): [1 Year, 1000km: The Oxford RobotCar Dataset](https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf) [IJRR 2016]
- [Redwood](http://redwood-data.org/3dscan/): [A large dataset of object scans](https://arxiv.org/pdf/1602.02481.pdf) [arXiv 2016]
- [S3DIS](http://buildingparser.stanford.edu/dataset.html): [3D Semantic Parsing of Large-Scale Indoor Spaces](https://openaccess.thecvf.com/content_cvpr_2016/papers/Armeni_3D_Semantic_Parsing_CVPR_2016_paper.pdf) [CVPR 2016], [Joint 2D-3D-Semantic Data for Indoor Scene Understanding](https://arxiv.org/pdf/1702.01105.pdf) [`seg`; arXiv 2017]
- [3DMatch](http://3dmatch.cs.princeton.edu/): [3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions](https://arxiv.org/pdf/1603.08182.pdf) [CVPR 2017]
- [SUNCG](https://sscnet.cs.princeton.edu/): [Semantic Scene Completion from a Single Depth Image](https://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Semantic_Scene_Completion_CVPR_2017_paper.pdf) [CVPR 2017]
- [ScanNet](http://www.scan-net.org/): [Scannet: Richly-annotated 3d reconstructions of indoor scenes](http://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.pdf) [`seg`, `det`; CVPR 2017 ]
- [Semantic3D](http://www.semantic3d.net/): [Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark](https://arxiv.org/pdf/1704.03847.pdf) [arXiv 2017]
- [SemanticKITTI](http://semantic-kitti.org/): [SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences](https://arxiv.org/pdf/1904.01416.pdf) [`seg`; ICCV 2019]
- [ScanObjectNN](https://hkust-vgd.github.io/scanobjectnn/): [Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data](https://arxiv.org/pdf/1908.04616.pdf) [ICCV 2019]
- [PartNet](https://cs.stanford.edu/~kaichun/partnet/): [PartNet: A Large-scale Benchmark for Fine-grained and
Hierarchical Part-level 3D Object Understanding](https://arxiv.org/pdf/1812.02713.pdf) [CVPR 2019]
- [Completion3D](https://completion3d.stanford.edu/): [TopNet: Structural Point Cloud Decoder](https://openaccess.thecvf.com/content_CVPR_2019/papers/Tchapmi_TopNet_Structural_Point_Cloud_Decoder_CVPR_2019_paper.pdf) [`completion`; CVPR 2019]
- [Argoverses](https://www.argoverse.org/): [Argoverse: 3D Tracking and Forecasting with Rich Maps](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf) [CVPR 2019]
- [Waymo Open Dataset](https://github.com/waymo-research/waymo-open-dataset): [Scalability in Perception for Autonomous Driving: Waymo Open Dataset](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf) [CVPR 2020]
- [nuScenes](https://www.nuscenes.org): [nuScenes: A multimodal dataset for autonomous driving](https://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf) [`det`, `tracking`; CVPR 2020]
- [SensatUrban](https://github.com/QingyongHu/SensatUrban): [Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges](https://arxiv.org/pdf/2009.03137.pdf) [CVPR 2021], [SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds](https://arxiv.org/pdf/2201.04494.pdf) [IJCV 2022]
- [WAYMO OPEN MOTION DATASET](https://arxiv.org/abs/2104.10133): [Large Scale Interactive Motion Forecasting for Autonomous Driving : The WAYMO OPEN MOTION DATASET](https://arxiv.org/pdf/2104.10133.pdf) [arXiv 2104]
- [Panoptic nuScenes](https://nuscenes.org/panoptic): [Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking](https://arxiv.org/pdf/2109.03805.pdf) [arXiv 2109]
- [BuildingNet](https://buildingnet.org/): [BuildingNet: Learning to Label 3D Buildings](https://arxiv.org/pdf/2110.04955.pdf) [ICCV 2021 Oral]
- [ARKitScenes](https://github.com/apple/ARKitScenes): [ARKitScenes - A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data](https://arxiv.org/pdf/2111.08897.pdf) [NeurIPS 2021]
